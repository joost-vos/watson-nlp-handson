{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training sentiment analysis model using Watson NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train sentiment analysis model on book reviews in the Dutch language using Watson NLP.\n",
    "\n",
    "The data that is used in this notebook is taken from the `Dutch Book Reviews Dataset`, https://github.com/benjaminvdb/DBRD with license [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/): Public Domain. This dataset is derived from the [Hebban - voor lezers door lezers](https://www.hebban.nl/), containing over 4.3 million book reviews and ratings. The dataset has been downsampled and prepared for fast execution of the notebook. You can download the downsampled and prepared data from [Nederlandstalige Watson NLP GitHub Repo](https://github.com/joost-vos/watson-nlp-handson/blob/main/boek_reviews_xs.csv)\n",
    "\n",
    "### What you'll learn in this notebook\n",
    "Watson NLP offers so-called blocks for various NLP tasks. This notebook shows:\n",
    "\n",
    "- **Sentiment analysis** with the _Sentiment block_ (`BERT Document Sentiment block`). Sentiment analysis classifies the sentiment of the reviews into positive or negative  sentiment. You will use the Sentiment workflow to train a BERT based sentiment analysis model. Then you will save the trained sentiment analysis model and finally evaluated the trained model on the test dataset for book reviews in the Dutch language\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1.  [Before you start](#beforeYouStart)\n",
    "1.  [Data Loading](#loadData)\n",
    "1.  [Data Processing](#processData)\n",
    "1.  [Model Building](#buildModel)\n",
    "    1. [Train sentiment analysis model using workflow](#trainWorkflow)\n",
    "    1. [Save the model/workflow](#saveModel)\n",
    "1.  [Model Evaluation](#evaluateModel)\n",
    "1.  [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beforeYouStart\"></a>\n",
    "## 1. Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Stop kernel of other notebooks.</b></div>\n",
    "\n",
    "**Note:** If you have other notebooks currently running with the _Default Python 3.10 + Watson NLP XS_ environment, **stop their kernels** before running this notebook. All these notebooks share the same runtime environment, and if they are running in parallel, you may encounter memory issues. To stop the kernel of another notebook, open that notebook, and select _File > Stop Kernel_.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Set Project token.</b></div>\n",
    "\n",
    "Before you can begin working on this notebook in Watson Studio in Cloud Pak for Data as a Service, you need to ensure that the project token is set so that you can access the project assets via the notebook.\n",
    "\n",
    "When this notebook is added to the project, a project access token should be inserted at the top of the notebook in a code cell. If you do not see the cell above, add the token to the notebook by clicking **More > Insert project token** from the notebook action bar.  By running the inserted hidden code cell, a project object is created that you can use to access project resources.\n",
    "\n",
    "![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Cell execution</div>\n",
    "\n",
    "Note that you can step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing and initializing some helper libs that are used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, types\n",
    "import pandas as pd\n",
    "# we want to show large text snippets to be able to explore the relevant text\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watson_nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loadData\"></a>\n",
    "## 2. Data Loading (Book reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> If you want to carry out sentiment analysis on any other dataset, you should first upload the dataset into the project and then update the name of the file in the next cell</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the book reviews into a DataFrame.\n",
    "Either choose the extra small (xs) data set containing 1000 book reviews with labels (boek_reviews_xs.csv).\n",
    "    or\n",
    "choose the medium (M) data set containing 5000 book reviews with labels (boek_reviews_m.csv).\n",
    "\n",
    "The XS dataset takes roughly 2-3 minutes per training epoch, whereas the M dataset takes 30 minutes per training epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, types\n",
    "# import pandas as pd\n",
    "buffer = project.get_file(\"boek_reviews_xs.csv\") # or use the boek_reviews_m.csv file  - !note! Training takes much longer with the larger source file\n",
    "\n",
    "books_df = pd.read_csv(buffer)\n",
    "books_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"processData\"></a>\n",
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to convert the raw data into processed data which can be fed into the sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToList(x):\n",
    "    return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_prep(df):\n",
    "    df['weight'] = 1\n",
    "    df.rename(columns={'label': 'labels'}, inplace=True)\n",
    "    df = df[['text', 'weight', 'labels']]\n",
    "    df['labels'] = df['labels'].replace({0: 'negative', 1: 'positive'})\n",
    "    df['labels'] = df['labels'].apply(convertToList)\n",
    "    display(df.head(10))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_processed = input_data_prep(books_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into test & train. Train dataset will be used to train the sentiment classification model and test dataset will be used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_orig, test_orig = train_test_split(books_df, test_size=0.2)\n",
    "train, test = train_test_split(books_df_processed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './train_data.json'\n",
    "train.to_json(train_file, orient='records')\n",
    "    \n",
    "test_file = './test_data.json'\n",
    "test.to_json(test_file, orient='records')\n",
    "\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"buildModel\"></a>\n",
    "## 4. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trainWorkflow\"></a>\n",
    "## 4.A Train the sentiment analysis model using workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from watson_nlp.workflows.document_sentiment import BERT\n",
    "from watson_nlp.toolkit import bert_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Pretrained model resource \n",
    "pretrained_model_resource = watson_nlp.load(watson_nlp.download(\"pretrained-model_bert_multi_bert_multi_uncased\"))\n",
    "# Print the number of hidden layers\n",
    "bert_config = bert_utils.BertConfig.from_json_file(pretrained_model_resource.bert_config_path)\n",
    "print(\"num_hidden_layers in base model: {}\".format(bert_config.num_hidden_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 layers in the `pretrained-model_bert_multi_bert_multi_uncased` model. You can modify the number of layers and other hyperparameters as shown in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sentiment workflow model for Dutch\n",
    "sentiment_model = watson_nlp.download_and_load('sentiment-aggregated_cnn-workflow_nl_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dutch Syntax stock models\n",
    "print('Downloading Syntax')\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_nl_stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax_lang_code_map = {\"en\": syntax_model}\n",
    "syntax_lang_code_map = {\"nl\": syntax_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b></div>\n",
    "<span style=\"color:red\">The next cell is going to take a lot of time in training. You can reduce this argument `train_max_seq_length` to 32 or 64 for quicker training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyperparameters & arguments:\n",
    "\n",
    "- train_file: Name of the json file which will be used for training the model\n",
    "- test_file: Name of the json file which will be used for evaluating the model\n",
    "- syntax_lang_code_map: A dictionary with language and syntax model. `{\"en\": syntax_model}` in this case\n",
    "- pretrained_model_resource: The name of the pretrained model downloaded from Watson NLP library. `pretrained-model_bert_multi_bert_multi_uncased` in this case.\n",
    "- label_list: List of labels for sentiment classification. `['negative', 'neutral', 'positive']`\n",
    "- learning_rate: [Learning rate for the model](https://en.wikipedia.org/wiki/Learning_rate). `2e-5`\n",
    "- num_train_epochs: Number of times the learning algorithm will work through the entire training dataset. `5`\n",
    "- do_lower_case: Convert all text to lower case. `True`\n",
    "- train_max_seq_length: Maximum number of tokens that a training sequence can contain. `128`\n",
    "- train_batch_size: Training batch size. `32`\n",
    "- dev_batch_size: Validation batch size. `32`\n",
    "- predict_batch_size: Test batch size. `128`\n",
    "- predict_max_seq_length: Maximum number of tokens that a prediction sequence can contain. `64`\n",
    "- num_layers_to_remove: Number of layers to be removed from the base model. `2`\n",
    "- combine_approach: \"Mean\" will average score calculation directly while 'NON_NEUTRAL_MEAN' will ignore neutral sentences for average score calculation. `NON_NEUTRAL_MEAN`\n",
    "- keep_model_artifacts: Keep model artifacts. `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using the workflow with compression \n",
    "# Set number of layers to remove as 2\n",
    "\n",
    "bert_wkflow = BERT.train(\n",
    "              train_file,\n",
    "              test_file,\n",
    "              syntax_lang_code_map,\n",
    "              pretrained_model_resource,\n",
    "              label_list=['negative', 'neutral', 'positive'],\n",
    "              learning_rate=2e-5,\n",
    "              num_train_epochs=5,\n",
    "              do_lower_case=True,\n",
    "              train_max_seq_length=64,\n",
    "              train_batch_size=16, # was 32\n",
    "              dev_batch_size=16, # was 32\n",
    "              predict_batch_size=64, # was 128\n",
    "              predict_max_seq_length=128,\n",
    "              num_layers_to_remove=2,\n",
    "              combine_approach=\"NON_NEUTRAL_MEAN\",\n",
    "              keep_model_artifacts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT config has been updated and saved in pre_train artifacts\"\n",
    "bert_updated_config_path = \"bert_pretrain_artifact/bert_config.json\"\n",
    "bert_config_updated  = bert_utils.BertConfig.from_json_file(bert_updated_config_path)\n",
    "print(\"num_hidden_layers after compression: {}\".format(bert_config_updated.num_hidden_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe here that the number of layers after compression has reduced from 12 to 10 i.e 2 layers were reduced from the base model for faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"saveModel\"></a>\n",
    "## 4.B Save the workflow/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the Workflow in a given location\n",
    "model_path = 'bert_set/wkflow_xs' # model_path = 'bert_set/wkflow'\n",
    "bert_wkflow.save('bert_set/wkflow_xs') # bert_wkflow.save('bert_set/wkflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the saved workflow for predict\n",
    "wk_loaded = watson_nlp.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow with input as raw document and language code\n",
    "# raw_document = 'I have been looking for this film for ages because it is quite rare to find as it was one of the video nasties. I finally found it on DVD at the end of last year it is a very low budget movie The story is set around amazon jungle tribes that are living in fear of the devil. Laura Crawford is a model who is kidnapped by a gang of thugs while she is working in South America. They take her into the jungle Laura is guarded by some ridiculous native who calls himself \"The Devil\" she has to go though all unpleasant things until they are happy. Maidens are Chained up. The devil demonstrates eating flesh in a horrible manner. Peter Weston, is the devil hunter, who goes into the jungle to try and rescue her,'\n",
    "raw_document = \"Dit is voor mij het beste boek aller tijden. Kippenvel.\"\n",
    "sentiment = wk_loaded.run(raw_document, language_code=\"nl\")\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model on the Cloud Object Storage (COS) associated with the Watson Studio instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data('bert_wkflow_book_reviews_xs_5_epochs_nl', data=bert_wkflow.as_file_like_object(), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluateModel\"></a>\n",
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to extract text from the dataset and create a new dataframe with the corresponding sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(review_text):\n",
    "    # run the syntax model\n",
    "    syntax_result = syntax_model.run(review_text, parsers=('token', 'lemma', 'part_of_speech'))\n",
    "    sentiment_result = wk_loaded.run(review_text, sentence_sentiment=True, language_code=\"nl\")\n",
    "    \n",
    "    document_sentiment = sentiment_result.to_dict()['label']\n",
    "    return document_sentiment\n",
    "\n",
    "# Helper method to create a new dataframe with the corresponding sentiment\n",
    "def create_sentiment_dataframe(df):\n",
    "    sentiment = df['text'].apply(lambda text: extract_sentiment(text))\n",
    "    sentiment_df = pd.DataFrame()\n",
    "    sentiment_df['Document Sentiment'] = sentiment\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = test_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "sentiment_df = create_sentiment_dataframe(test_small)\n",
    "books_sentiment_df = test_small[['text', 'labels']].merge(sentiment_df[['Document Sentiment']], how='left', left_index=True, right_index=True)\n",
    "books_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom function to calculate accuracy. **Note**: model evaluation method is WIP for the workflow based model and will be released soon after which you will not need to write custom code for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "conditions = [\n",
    "    books_sentiment_df['labels'].eq(0) & books_sentiment_df['Document Sentiment'].eq('SENT_NEGATIVE'),\n",
    "    books_sentiment_df['labels'].eq(1) & books_sentiment_df['Document Sentiment'].eq('SENT_POSITIVE'),\n",
    "]\n",
    "\n",
    "choices = [1,1]\n",
    "\n",
    "books_sentiment_df['score'] = np.select(conditions, choices, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY SCORE:\", books_sentiment_df['score'].sum()/len(books_sentiment_df['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "books_sentiment_df['labels'] = books_sentiment_df['labels'].replace({0:'SENT_NEGATIVE', 1:'SENT_POSITIVE'})\n",
    "actual = books_sentiment_df['labels']\n",
    "# predicted values\n",
    "predicted = books_sentiment_df['Document Sentiment']\n",
    "\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(actual,predicted, labels=['SENT_POSITIVE','SENT_NEGATIVE'])\n",
    "print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=['SENT_POSITIVE','SENT_NEGATIVE']).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted,labels=['SENT_POSITIVE','SENT_NEGATIVE'])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">This notebook demonstrates how to fine-tune/re-train the BERT Sentiment workflow using compression. The model performance has improved over Out-of-the-box (OOTB) pre-trained model where accuracy was 87% and which is now 96% with the fine-tuned model. The model performance generally improves especially if you have nuances in the dataset that your model needs to learn from the dataset.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this content is made available by IBM Build Lab to foster Embedded AI technology adoption. The content may include systems & methods pending patent with USPTO and protected under US Patent Laws. For redistribution of this content, IBM will use release process. For any questions please log an issue in the hosting [GitHub repository](https://github.com/ibm-build-labs/Watson-NLP). \n",
    "\n",
    "Developed by IBM Build Lab \n",
    "\n",
    "Copyright - 2022-2023 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
