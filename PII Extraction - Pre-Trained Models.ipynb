{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Personal Identifiable Information (PII) using Watson NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use Case</h2>\n",
    "\n",
    "<b>PII extraction is the process of identifying and extracting personal information from a document or dataset</b>. This notebook demonstrates how to extract PII entities using Watson NLP Pre-trained models. This information can include **names, addresses, phone numbers, email addresses, Social Security numbers, Credit Card number, and other types of information** that can be used to identify an individual. \n",
    "\n",
    "\n",
    "<h2>What you'll learn in this notebook</h2>\n",
    "\n",
    "Watson NLP offers Pre-trained Models for various NLP tasks and also provides fine-tune functionality for custom training. This notebooks shows:\n",
    "\n",
    "* <b>RBR</b>:  A Rule-Based Reasoner (RBR) in NLP works by using a set of predefined rules to process and understand natural language input. These rules are used to identify specific patterns or structures in the input text and determine the meaning of the text based on those patterns.\n",
    "\n",
    "\n",
    "* <b>BILSTM</b>: the BiLSTM network would take the preprocessed text as input and learn to identify patterns and relationships between words that are indicative of PII data. The BiLSTM network would then output a probability score for each word in the text, indicating the likelihood that the word is part of a PII entity. The BiLSTM network may also be trained to recognize specific entities such as names, addresses, phone numbers, email addresses, etc.\n",
    "\n",
    "\n",
    "* <b>BERT</b>: Bidirectional Encoder Representations from Transformers (BERT) uses the Google Multilingual BERT model, meaning that a single model can analyze input texts from multiple languages. BERT uses a transformer-based neural network architecture that allows for bidirectional processing of input text, meaning that it can take into account both the context before and after a given word in a sentence. BERT can be used for entity extraction, which involves identifying and extracting important pieces of information (such as named entities) from unstructured text data. In the context of entity extraction, BERT can be fine-tuned on a labeled dataset of text data and entity labels (such as person, organization, location, or date). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Before you start](#beforeYouStart)\n",
    "1. [Load Entity PII Models](#LoadModel)\n",
    "1. [Data generation for Testing](#Loaddata)\n",
    "1. [Watson NLP Models](#NLPModels)    \n",
    "   1. [BiLSTM Pretrained](#BILSTMPre)\n",
    "   1. [RBR Pretrained](#RBRPre)\n",
    "   1. [BERT Pretrained](#Bert)\n",
    "1. [Testing Usecase](#Testing)  \n",
    "   1.  [RBR Model Testing for Caredit card Extraction, URL and Email](#5.1)\n",
    "       1. [RBR Stock - URL Extraction](#5.1.1)\n",
    "       1. [Combine RBR PII and RBR Stock (URL)](#5.1.2)\n",
    "   1. [EmailAddress Extraction using RBR](#5.2)\n",
    "1. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beforeYouStart\"></a>\n",
    "### 1. Before you start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Stop kernel of other notebooks.</b></div>\n",
    "\n",
    "**Note:** If you have other notebooks currently running with the _Default Python 3.x environment, **stop their kernels** before running this notebook. All these notebooks share the same runtime environment, and if they are running in parallel, you may encounter memory issues. To stop the kernel of another notebook, open that notebook, and select _File > Stop Kernel_.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Set Project token.</b></div>\n",
    "\n",
    "Before you can begin working on this notebook in Watson Studio in Cloud Pak for Data as a Service, you need to ensure that the project token is set so that you can access the project assets via the notebook.\n",
    "\n",
    "When this notebook is added to the project, a project access token should be inserted at the top of the notebook in a code cell. If you do not see the cell above, add the token to the notebook by clicking **More > Insert project token** from the notebook action bar.  By running the inserted hidden code cell, a project object is created that you can use to access project resources.\n",
    "\n",
    "![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Cell execution</div>\n",
    "\n",
    "Note that you can step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import watson_nlp\n",
    "import random\n",
    "from faker import Faker\n",
    "from watson_nlp import data_model as dm\n",
    "from watson_nlp.toolkit.entity_mentions_utils import prepare_train_from_json\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence Tensorflow warnings\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoadModel\"></a>\n",
    "### 2. Load Entity PII Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"watson_nlp.load()\" is used to download a pre-trained NLP model, such as IBM Watson NLP, which can then be used to process and analyze text data. This function allows you to quickly and easily integrate advanced NLP capabilities into your application, without having to train a model from scratch. By downloading a pre-trained model, you can save time and resources, and benefit from the expertise of the model developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the models catalog here - https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/watson-nlp-block-catalog.html?context=cpdaas&audience=wdp\n",
    "\n",
    "# Load a syntax model to split the text into sentences and tokens\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_nl_stock'))\n",
    "# Load bilstm model in WatsonNLP\n",
    "# Note that we are loading the English PII BiLSTM model as there is no Dutch model available yet - the English model works, but will result in lower confidence scores as you will see\n",
    "bilstm_model = watson_nlp.load(watson_nlp.download('entity-mentions_bilstm_en_pii'))\n",
    "# Load rbr model in WatsonNLP\n",
    "rbr_model = watson_nlp.load(watson_nlp.download('entity-mentions_rbr_multi_pii'))\n",
    "# BERT Load BERT entity model in WatsonNLP\n",
    "bert_entity_model = watson_nlp.load(watson_nlp.download('entity-mentions_bert_multi_stock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Loaddata\"></a>\n",
    "### 3. Data generation for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data __Name__, __SSN__ and __credit card numbers__ using faker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faker is used to generate fake data for testing purposes\n",
    "fake = Faker(locale='nl_NL')\n",
    "\n",
    "def format_data():  \n",
    "        #Generate a random\n",
    "        name = fake.name() \n",
    "\n",
    "        #Generate a random SSN \n",
    "        ssn = fake.ssn()\n",
    "\n",
    "        #Generate a random CCN \n",
    "        ccn = fake.credit_card_number()\n",
    "\n",
    "        text_1 = \"\"\"Mijn naam is %s, en mijn BSN is %s. Hier is het nummer van mijn credit card %s\"\"\" % (name, ssn, ccn)\n",
    "\n",
    "        text_2 = \"\"\"%s is mijn burgerservicenummer. De naam op mijn creditcard %s is %s.\"\"\"% (ssn, ccn, name)\n",
    "\n",
    "        text_3 = \"\"\"Mijn creditcardnummer is %s en burgerservicenummer is %s, ik ben %s\"\"\" %(ccn,ssn,name)\n",
    "\n",
    "        print(text_1)\n",
    "        print(text_2)\n",
    "        print(text_3)        \n",
    "\n",
    "        text = random.choice([text_1, text_2,text_3])\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = format_data()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"NLPModels\"></a>\n",
    "### 4.  Watson NLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"BILSTMPre\"></a>\n",
    "### 4. 1 BiLSTM Pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"pretrained\" refers to a pre-trained BiLSTM model, which has already been trained on a large corpus of text data and can be __fine-tuned__ or used __as-is__ for specific NLP tasks, such as sentiment analysis, named entity recognition, and so on. By using a pretrained BiLSTM model, you can __leverage the knowledge learned from the training data to quickly build NLP applications with improved accuracy__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(bilstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Pretrained bilstm_model model in WatsonNLP\n",
    "syntax_result = syntax_model.run(text)\n",
    "bilstm_result = bilstm_model.run(syntax_result)\n",
    "print(bilstm_result)\n",
    "for i in bilstm_result.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RBRPre\"></a>\n",
    "\n",
    "### 4.2 RBR Pretrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretrained rule-based model is a model that has already been trained on a large corpus of text data and has a set of predefined rules for processing text data. By using a pretrained rule-based model, you can leverage the knowledge learned from the training data to quickly build NLP applications with improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= format_data()\n",
    "text2= format_data()\n",
    "text3= format_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test=[text1,text2,text3]\n",
    "all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(rbr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=1\n",
    "# Test the pretrain\n",
    "for test in all_test:\n",
    "    rbr_result = rbr_model.run(test, language_code='nl')\n",
    "    print(rbr_result)\n",
    "    \n",
    "    for i in rbr_result.mentions:\n",
    "        print(\"Text\",t, i.span.text.ljust(15, \" \"), \"Type: \", i.type)\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Bert\"></a>\n",
    "\n",
    "### 4.3 Entity Mentions BERT multilang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset for URL extraction \n",
    "test1 = \"Mijn naam is Robert van Linschoten. Ik ben geboren te Haarlem en mijn burgerservicenummer is 574106984. Hier is het nummer van mijn creditcard is 213117117387576, http://www.example.com/page_id=5555555555554444\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Pretrained bert_model model in WatsonNLP\n",
    "syntax_prediction = syntax_model.run(test1)\n",
    "bert_result = bert_entity_model.run(syntax_prediction)\n",
    "print(bert_result)\n",
    "for i in bert_result.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Testing\"></a>\n",
    "\n",
    "## 5. Testing Usecase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "### 5.1 RBR Model Testing for Credit card Extraction, URL and Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.1\"></a>\n",
    "#### 5.1.1 RBR Stock (URL Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Pretrained rbr stock model in WatsonNLP\n",
    "rbr_ent_model = watson_nlp.load(watson_nlp.download('entity-mentions_rbr_nl_stock'))\n",
    "rbr_ent_result = rbr_ent_model.run(test1)\n",
    "\n",
    "for i in rbr_ent_result.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Pretrained rbr PII model in WatsonNLP\n",
    "\n",
    "rbr_result_pii = rbr_model.run(test1, language_code='nl')\n",
    "rbr_result_pii\n",
    "\n",
    "for i in rbr_result_pii.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.2\"></a>\n",
    "#### 5.1.2 Combine RBR PII and RBR Stock (URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to be one object\n",
    "combined_mentions = rbr_ent_result + rbr_result_pii\n",
    "combined_mentions\n",
    "\n",
    "for i in combined_mentions.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "### 5.2 EmailAddress Extraction using RBR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset for EMail extraction \n",
    "text = format_data() +\" my maild id is sample.Email@gmail.com\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Pretrained rbr PII model in WatsonNLP\n",
    "\n",
    "rbr_result_pii = rbr_model.run(text, language_code='nl')\n",
    "rbr_result_pii\n",
    "\n",
    "for i in rbr_result_pii.mentions:\n",
    "    print(\"Text: \", i.span.text.ljust(15, \" \"), \"Type: \", i.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary\n",
    "\n",
    "<span style=\"color:blue\">This notebook shows you how to use the Watson NLP library to:\n",
    "1. Extract PII using Pre-trained Models\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this content is made available by IBM Build Lab to foster Embedded AI technology adoption. The content may include systems & methods pending patent with USPTO and protected under US Patent Laws. For redistribution of this content, IBM will use release process. For any questions please log an issue in the GitHub.\n",
    "\n",
    "Developed by IBM Build Lab\n",
    "\n",
    "Copyright - 2023 IBM Corporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
